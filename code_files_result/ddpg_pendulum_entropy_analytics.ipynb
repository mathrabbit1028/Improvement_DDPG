{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1P1SQ9Q1a0olUnMnUpq4Nilk27OJvS9Qf","timestamp":1688888540877}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yw71Zr6ehe7y","executionInfo":{"status":"ok","timestamp":1689055073324,"user_tz":-540,"elapsed":21010,"user":{"displayName":"23 1","userId":"14049029142865052059"}},"outputId":"51bf273a-e983-4f2a-9ed1-9d1ee14403ba"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# This mounts your Google Drive to the Colab VM.\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["FOLDERNAME = 'Colab Notebooks/analytics/결과'\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","import sys\n","sys.path.append('/content/drive/MyDrive/{}'.format(FOLDERNAME))\n","\n","# Change dariectory to current folder\n","%cd /content/drive/MyDrive/$FOLDERNAME"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_tMvx2Qvh70b","executionInfo":{"status":"ok","timestamp":1689055074046,"user_tz":-540,"elapsed":725,"user":{"displayName":"23 1","userId":"14049029142865052059"}},"outputId":"f3a587d1-8a30-49d0-8459-7ed9a99a69f6"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1ZfbeRJGYGC-9d0cU_aG1Cprzhc40b-7T/analytics/결과\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4eoGnZwSjpnG","executionInfo":{"status":"ok","timestamp":1689055078181,"user_tz":-540,"elapsed":4138,"user":{"displayName":"23 1","userId":"14049029142865052059"}},"outputId":"52118317-5ee6-4c6e-dd2b-8ab3b1b6742a"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/jaxlib/xla_client.py:225: DeprecationWarning: ml_dtypes.float8_e4m3b11 is deprecated. Use ml_dtypes.float8_e4m3b11fnuz\n","  float8_e4m3b11fnuz = ml_dtypes.float8_e4m3b11\n"]}],"source":["import gym\n","import tensorflow as tf\n","from tensorflow.keras import layers\n","import numpy as np\n","import matplotlib.pyplot as plt\n","tf.config.run_functions_eagerly(True)"]},{"cell_type":"code","source":["problem = \"Pendulum-v1\"\n","env = gym.make(problem)\n","\n","num_states = env.observation_space.shape[0]\n","print(\"Size of State Space ->  {}\".format(num_states))\n","num_actions = env.action_space.shape[0]\n","print(\"Size of Action Space ->  {}\".format(num_actions))\n","\n","upper_bound = env.action_space.high[0]\n","lower_bound = env.action_space.low[0]\n","\n","print(\"Max Value of Action ->  {}\".format(upper_bound))\n","print(\"Min Value of Action ->  {}\".format(lower_bound))\n","\n","critic_value_list=[]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-ldl7IVTjv-Y","executionInfo":{"status":"ok","timestamp":1689055078181,"user_tz":-540,"elapsed":15,"user":{"displayName":"23 1","userId":"14049029142865052059"}},"outputId":"68d11bec-07f0-4308-e0d1-b81160cd508c"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Size of State Space ->  3\n","Size of Action Space ->  1\n","Max Value of Action ->  2.0\n","Min Value of Action ->  -2.0\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n"]}]},{"cell_type":"code","source":["class OUActionNoise:\n","    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n","        self.theta = theta\n","        self.mean = mean\n","        self.std_dev = std_deviation\n","        self.dt = dt\n","        self.x_initial = x_initial\n","        self.reset()\n","\n","    def __call__(self):\n","        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n","        x = (\n","            self.x_prev\n","            + self.theta * (self.mean - self.x_prev) * self.dt\n","            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n","        )\n","        # Store x into x_prev\n","        # Makes next noise dependent on current one\n","        self.x_prev = x\n","        return x\n","\n","    def reset(self):\n","        if self.x_initial is not None:\n","            self.x_prev = self.x_initial\n","        else:\n","            self.x_prev = np.zeros_like(self.mean)"],"metadata":{"id":"EO6V2Ed_j2ug","executionInfo":{"status":"ok","timestamp":1689055078181,"user_tz":-540,"elapsed":2,"user":{"displayName":"23 1","userId":"14049029142865052059"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["class Buffer:\n","    def __init__(self, buffer_capacity=100000, batch_size=64):\n","        # Number of \"experiences\" to store at max\n","        self.buffer_capacity = buffer_capacity\n","        # Num of tuples to train on.\n","        self.batch_size = batch_size\n","\n","        # Its tells us num of times record() was called.\n","        self.buffer_counter = 0\n","\n","        # Instead of list of tuples as the exp.replay concept go\n","        # We use different np.arrays for each tuple element\n","        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n","        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n","        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n","        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n","\n","    # Takes (s,a,r,s') obervation tuple as input\n","    def record(self, obs_tuple):\n","        # Set index to zero if buffer_capacity is exceeded,\n","        # replacing old records\n","        index = self.buffer_counter % self.buffer_capacity\n","\n","        self.state_buffer[index] = obs_tuple[0]\n","        self.action_buffer[index] = obs_tuple[1]\n","        self.reward_buffer[index] = obs_tuple[2]\n","        self.next_state_buffer[index] = obs_tuple[3]\n","\n","        self.buffer_counter += 1\n","\n","    # Eager execution is turned on by default in TensorFlow 2. Decorating with tf.function allows\n","    # TensorFlow to build a static graph out of the logic and computations in our function.\n","    # This provides a large speed up for blocks of code that contain many small TensorFlow operations such as this one.\n","    @tf.function\n","    def update(\n","        self, state_batch, action_batch, reward_batch, next_state_batch,\n","    ):\n","        # Training and updating Actor & Critic networks.\n","        # See Pseudo Code.\n","        with tf.GradientTape() as tape:\n","            target_actions = target_actor(next_state_batch, training=True)\n","            y = reward_batch + gamma * target_critic(\n","                [next_state_batch, target_actions], training=True\n","            )\n","            critic_value = critic_model([state_batch, action_batch], training=True)\n","            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n","\n","        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n","        critic_optimizer.apply_gradients(\n","            zip(critic_grad, critic_model.trainable_variables)\n","        )\n","\n","        with tf.GradientTape() as tape:\n","            actions = actor_model(state_batch, training=True)\n","            critic_value = critic_model([state_batch, actions], training=True)\n","\n","            critic_value = -critic_value * np.log(np.abs(critic_value / target_critic([next_state_batch, target_actions], training=True) / len(critic_value)))\n","\n","            # Used `-value` as we want to maximize the value given\n","            # by the critic for our actions\n","            actor_loss = -tf.math.reduce_mean(critic_value)\n","            # print(\"loss:\", actor_loss)\n","\n","        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n","        actor_optimizer.apply_gradients(\n","            zip(actor_grad, actor_model.trainable_variables)\n","        )\n","\n","        # \"deleted\" basic code\n","\n","        # with tf.GradientTape() as tape:\n","        #     actions = actor_model(state_batch, training=True)\n","        #     critic_value = critic_model([state_batch, actions], training=True)\n","\n","        #     # Used `-value` as we want to maximize the value given\n","        #     # by the critic for our actions\n","        #     actor_loss = -tf.math.reduce_mean(critic_value)\n","\n","        # actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n","        # actor_optimizer.apply_gradients(\n","        #     zip(actor_grad, actor_model.trainable_variables)\n","        # )\n","\n","    # We compute the loss and update parameters\n","    def learn(self):\n","        # Get sampling range\n","        record_range = min(self.buffer_counter, self.buffer_capacity)\n","        # Randomly sample indices\n","        batch_indices = np.random.choice(record_range, self.batch_size)\n","\n","        # Convert to tensors\n","        critic_value_list.append(self.state_buffer[batch_indices])\n","        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n","        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n","        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n","        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n","        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n","\n","        self.update(state_batch, action_batch, reward_batch, next_state_batch)\n","\n","\n","# This update target parameters slowly\n","# Based on rate `tau`, which is much less than one.\n","@tf.function\n","def update_target(target_weights, weights, tau):\n","    for (a, b) in zip(target_weights, weights):\n","        a.assign(b * tau + a * (1 - tau))"],"metadata":{"id":"yv1p11Vwj5_5","executionInfo":{"status":"ok","timestamp":1689055079102,"user_tz":-540,"elapsed":6,"user":{"displayName":"23 1","userId":"14049029142865052059"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def get_actor():\n","    # Initialize weights between -3e-3 and 3-e3\n","    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n","\n","    inputs = layers.Input(shape=(num_states,))\n","    out = layers.Dense(256, activation=\"relu\")(inputs)\n","    out = layers.Dense(256, activation=\"relu\")(out)\n","    outputs = layers.Dense(1, activation=\"tanh\", kernel_initializer=last_init)(out)\n","\n","    # Our upper bound is 2.0 for Pendulum.\n","    outputs = outputs * upper_bound\n","    model = tf.keras.Model(inputs, outputs)\n","    return model\n","\n","\n","def get_critic():\n","    # State as input\n","    state_input = layers.Input(shape=(num_states))\n","    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n","    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n","\n","    # Action as input\n","    action_input = layers.Input(shape=(num_actions))\n","    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n","\n","    # Both are passed through seperate layer before concatenating\n","    concat = layers.Concatenate()([state_out, action_out])\n","\n","    out = layers.Dense(256, activation=\"relu\")(concat)\n","    out = layers.Dense(256, activation=\"relu\")(out)\n","    outputs = layers.Dense(1)(out)\n","\n","    # Outputs single value for give state-action\n","    model = tf.keras.Model([state_input, action_input], outputs)\n","\n","    return model"],"metadata":{"id":"YJdZQPtMj8es","executionInfo":{"status":"ok","timestamp":1689055079102,"user_tz":-540,"elapsed":5,"user":{"displayName":"23 1","userId":"14049029142865052059"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def policy(state, noise_object):\n","    sampled_actions = tf.squeeze(actor_model(state))\n","    noise = noise_object()\n","    # Adding noise to action\n","    sampled_actions = sampled_actions.numpy() + noise\n","\n","    # We make sure action is within bounds\n","    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n","\n","    return [np.squeeze(legal_action)]"],"metadata":{"id":"u5qzxzvkj_lr","executionInfo":{"status":"ok","timestamp":1689055079102,"user_tz":-540,"elapsed":4,"user":{"displayName":"23 1","userId":"14049029142865052059"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","from datetime import datetime\n","def main(total_episodes):\n","  global actor_model, critic_model, target_actor, target_critic, critic_optimizer\n","  global actor_optimizer, gamma\n","  std_dev = 0.2\n","  ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n","\n","  actor_model = get_actor()\n","  critic_model = get_critic()\n","\n","  target_actor = get_actor()\n","  target_critic = get_critic()\n","\n","  # Making the weights equal initially\n","  target_actor.set_weights(actor_model.get_weights())\n","  target_critic.set_weights(critic_model.get_weights())\n","\n","  # Learning rate for actor-critic models\n","  critic_lr = 0.002\n","  actor_lr = 0.001\n","\n","  critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n","  actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n","\n","  # Discount factor for future rewards\n","  gamma = 0.99\n","  # Used to update target networks\n","  tau = 0.005\n","\n","  buffer = Buffer(50000, 64)\n","\n","\n","  # To store reward history of each episode\n","  ep_reward_list = []\n","  # To store time history of each episode\n","  datetime_list=[]\n","  start = datetime.now()\n","\n","  #출력 모아놓는 코드\n","  a=[]\n","\n","  # Takes about 9 min to train\n","  for ep in range(total_episodes):\n","\n","      prev_state = env.reset()\n","      episodic_reward = 0\n","\n","      while True:\n","          # Uncomment this to see the Actor in action\n","          # But not in a python notebook.\n","          # env.render()\n","\n","          tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n","\n","          action = policy(tf_prev_state, ou_noise)\n","          # Recieve state and reward from environment.\n","          state, reward, done, info = env.step(action)\n","\n","          buffer.record((prev_state, action, reward, state))\n","          episodic_reward += reward\n","\n","          buffer.learn()\n","          update_target(target_actor.variables, actor_model.variables, tau)\n","          update_target(target_critic.variables, critic_model.variables, tau)\n","\n","          # End this episode when `done` is True\n","          if done:\n","              break\n","\n","          prev_state = state\n","\n","      # 끝난 시간 얻기\n","      end = datetime.now()\n","\n","      ep_reward_list.append(episodic_reward)\n","\n","      datetime_list.append(end-start)\n","\n","  return datetime_list, ep_reward_list\n","\n","import csv\n","\n","number_of_episode=50\n","n=20\n","\n","indexing_str=datetime.today().strftime(\"%Y%m%d_%H%M%S\")\n","for _ in tqdm(range(n)):\n","    with open(f'list_to_csv_entropy-{number_of_episode}{indexing_str}.csv','a',newline='') as f:\n","      writer = csv.writer(f)\n","      data_for_save=main(number_of_episode) #totalepisode\n","      writer.writerow(data_for_save[0])\n","      writer.writerow(data_for_save[1])"],"metadata":{"id":"69gutxZZkFl1","colab":{"base_uri":"https://localhost:8080/"},"outputId":"42833876-d1db-44a9-f4d3-a28d5044103f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":[" 65%|██████▌   | 13/20 [3:40:59<1:58:30, 1015.80s/it]"]}]}]}